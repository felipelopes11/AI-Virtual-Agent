‚òï Barista Virtual - Agente de Atendimento com IA
üìù Descri√ß√£o
O Barista Virtual √© uma aplica√ß√£o full-stack que simula um assistente de IA especialista para um e-commerce de caf√©s especiais. O agente √© capaz de responder a perguntas detalhadas sobre os produtos, baseando as suas respostas exclusivamente num cat√°logo de produtos fornecido, evitando "alucina√ß√µes" e garantindo precis√£o.

Este projeto foi constru√≠do para demonstrar a aplica√ß√£o pr√°tica de Modelos de Linguagem Grandes (LLMs) numa solu√ß√£o de neg√≥cio, com foco em IA local, privacidade de dados e uma arquitetura moderna.

‚ú® Funcionalidades Principais
Chat em Tempo Real: Interface de chat reativa constru√≠da com React.

IA Especialista (RAG): Utiliza a t√©cnica de Retrieval-Augmented Generation para fornecer respostas baseadas num cat√°logo de produtos espec√≠fico.

Respostas em Streaming: A resposta da IA √© exibida palavra por palavra, melhorando a experi√™ncia do utilizador.

IA Local e Privada: Executa o modelo de linguagem Llama 3 localmente com o Ollama, garantindo que nenhum dado sens√≠vel √© enviado para APIs externas.

API Robusta: Backend constru√≠do com FastAPI, garantindo alto desempenho e documenta√ß√£o autom√°tica.

üöÄ Tecnologias Utilizadas
Backend
Linguagem: Python

Framework API: FastAPI

Servidor: Uvicorn

Frontend
Biblioteca: React

Ferramenta de Constru√ß√£o: Vite

Comunica√ß√£o API: Axios

Intelig√™ncia Artificial
Modelo (LLM): Llama 3 (via Ollama)

Orquestra√ß√£o: LangChain

Base de Dados Vetorial: ChromaDB

T√©cnica: RAG (Retrieval-Augmented Generation)

üõ†Ô∏è Como Executar o Projeto Localmente
Pr√©-requisitos:

Python 3.10+

Node.js e npm

Ollama instalado e a correr

1. Clonar o Reposit√≥rio

git clone [https://github.com/felipelopes11/Agente-Virtual-IA.git](https://github.com/felipelopes11/Agente-Virtual-IA.git)
cd Agente-Virtual-IA

2. Configurar o Backend

# Navegar para a pasta do backend
cd backend

# Criar e ativar o ambiente virtual
python -m venv venv
# No Windows:
.\\venv\\Scripts\\activate
# No macOS/Linux:
# source venv/bin/activate

# Instalar as depend√™ncias
pip install -r requirements.txt

# Descarregar o modelo de IA (se ainda n√£o o tiver)
ollama pull llama3:8b

# Iniciar o servidor
python -m uvicorn app.main:app --reload

O backend estar√° a ser executado em http://localhost:8000.

3. Configurar o Frontend

# Num novo terminal, navegar para a pasta do frontend
cd frontend

# Instalar as depend√™ncias
npm install

# Iniciar o servidor de desenvolvimento
npm run dev

A aplica√ß√£o estar√° acess√≠vel em http://localhost:5173.
